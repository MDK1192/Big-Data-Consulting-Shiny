from torch import optim
from PIL import Image
from torchvision import datasets, transforms, models
from collections import OrderedDict
#from os import walk
from pathlib import Path
from torch.utils.data import Dataset, DataLoader
video_path
audio <- "www/current_files/audio"
video <- "www/current_files/video"
reticulate::repl_python()
import numpy as np
import pandas as pd
pd.set_option('display.expand_frame_repr', False)
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential, load_model, save_model
import torch
import json
import os
from torch import nn
from torch import optim
from PIL import Image
from torchvision import datasets, transforms, models
from collections import OrderedDict
#from os import walk
from pathlib import Path
from torch.utils.data import Dataset, DataLoader
PATH_TO_FILE = Path.cwd() / path
path = "www/current_files/video"
PATH_TO_FILE = Path.cwd() / path
files = os.listdir(PATH_TO_FILE)
classification = []
percentages_0 = []
percentages_1 = []
percentages_2 = []
percentages_3 = []
percentages_4 = []
percentages_5 = []
percentages_6 = []
model = tf.keras.models.load_model("video_model.h5")
library(reticulate)
remove.packages("reticulate", lib="~/R/win-library/3.6")
install.packages("reticulate")
library(reticulate)
try(source_python("skript_video_class.py"))
Y
library(shiny)
library(shinyFiles)
library(shinyjs)
library(shinydashboard)
library(reticulate)
library(imager)
library(DT)
library(reader)
library(dplyr)
library(av)
library(plotly)
library(shinyalert)
try(source_python("skript_video_class.py"))
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
try(source_python("skript_video_class.py"))
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
try(source_python("skript_audio_class.py"))
try(source_python("skript_video_class.py"))
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_audio <- check_wav(audio))
runApp()
audio_path <<- audio_path
video_path <<- video_path
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
py_install("pillow")
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
reticulate::repl_python()
from PIL import Image
conda_install("pillow")
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
reticulate::repl_python()
from PIL import Image
image_list[i]=keras.preprocessing.image.load_img(PATH_TO_IMAGE, target_size=(64, 64))
quit
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
library(shiny)
library(shinyFiles)
library(shinyjs)
library(shinydashboard)
library(reticulate)
library(imager)
library(DT)
library(reader)
library(dplyr)
library(av)
library(plotly)
library(shinyalert)
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
try(df_video <- check_img(video_path))
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
reticulate::repl_python()
from PIL import Image
sys.modules['Image'] = Image
from torchvision import datasets, transforms, models
from collections import OrderedDict
from os import walk
from pathlib import Path
from torch.utils.data import Dataset, DataLoader
import shutil
quit
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
try(source_python("skript_audio_class.py"))
try(source_python("skript_video_class.py"))
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
try(source_python("skript_video_class.py"))
pip install scipy
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
try(source_python("skript_video_class.py"))
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
reticulate::repl_python()
from PIL import Image
# -*- coding: utf-8 -*-
"""
Created on Sat Dec  5 11:05:46 2020
@author: Marc
"""
# Imports here
import numpy as np
import pandas as pd
pd.set_option('display.expand_frame_repr', False)
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential, load_model, save_model
import torch
import json
import os
from torch import nn
from torch import optim
#from PIL import Image
from IPython.display import display
from PIL import Image
#import PIL.Image
from torchvision import datasets, transforms, models
from collections import OrderedDict
#from os import walk
from pathlib import Path
from torch.utils.data import Dataset, DataLoader
PATH_TO_FILE = Path.cwd() / path
path = "www/current_files/video"
PATH_TO_FILE = Path.cwd() / path
files = os.listdir(PATH_TO_FILE)
classification = []
percentages_0 = []
percentages_1 = []
percentages_2 = []
percentages_3 = []
percentages_4 = []
percentages_5 = []
percentages_6 = []
model = tf.keras.models.load_model("video_model.h5")
image_list=os.listdir(PATH_TO_FILE)
for i in range(len(image_list)):
PATH_TO_IMAGE = PATH_TO_FILE / image_list[i]
print(PATH_TO_IMAGE)
image_list[i]=keras.preprocessing.image.load_img(PATH_TO_IMAGE, target_size=(64, 64))
class_names = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']
import PIL.Image
image_list=os.listdir(PATH_TO_FILE)
for i in range(len(image_list)):
PATH_TO_IMAGE = PATH_TO_FILE / image_list[i]
print(PATH_TO_IMAGE)
image_list[i]=keras.preprocessing.image.load_img(PATH_TO_IMAGE, target_size=(64, 64))
class_names = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']
result_list = []
conda_install("r-reticulate", "pillow")
conda_remove("r-reticulate", "pillow")
conda_install("r-reticulate", "pillow")
py_install("pillow")
py_uninstall("pillow")
py_install("pillow")
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
conda_install("r-reticulate", "Pillow")
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
runApp()
conda_install("r-reticulate", "pillow")
conda_remove("r-reticulate", "pillow")
conda_install("r-reticulate", "pillow")
use_condaenv("r-reticulate")
runApp()
runApp()
runApp()
#Laden von TransformationSkript via reticulate
try(source_python("skript_wav_png_transform.py"))
#Laden von TransformationSkript via reticulate
try(source_python("skript_wav_png_transform.py"))
#Laden von TransformationSkript via reticulate
try(source_python("skript_wav_png_transform.py"))
#.wav dateien identifizieren und selektieren
path_of_file <- strsplit(as.character(df_files$Herkunft), "www/")
shiny::runApp()
#Laden von TransformationSkript via reticulate
try(source_python("skript_wav_png_transform.py"))
reticulate::repl_python()
import librosa
reticulate::repl_python()
import librosa
library(reticulate)
reticulate::repl_python()
import librosa
reticulate::repl_python()
from pathlib import Path
reticulate::repl_python()
import numpy as np
reticulate::repl_python()
import matplotlib
reticulate::repl_python()
matplotlib.use('Agg')
reticulate::repl_python()
import matplotlib.pyplot as plt
reticulate::repl_python()
import os
reticulate::repl_python()
from PyQt5 import QtCore, QtGui, QtWidgets
reticulate::repl_python()
import numpy as np
library(shiny)
library(shinyFiles)
library(shinyjs)
library(shinydashboard)
library(reticulate)
library(imager)
library(DT)
library(reader)
library(dplyr)
library(av)
library(plotly)
library(shinyalert)
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
try(source_python("skript_video_class.py"))
try(source_python("skript_video_class.py"))
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
try(source_python("skript_video_class.py"))
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
try(source_python("skript_video_class.py"))
try(source_python("skript_video_class.py"))
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
#Laden von TransformationSkript via reticulate
try(source_python("skript_wav_png_transform.py"))
#Laden von TransformationSkript via reticulate
try(source_python("skript_wav_png_transform.py"))
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
try(source_python("skript_video_class.py"))
library(shiny)
library(shinyFiles)
library(shinyjs)
library(shinydashboard)
library(reticulate)
library(imager)
library(DT)
library(reader)
library(dplyr)
library(av)
library(plotly)
library(shinyalert)
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
#Laden von TransformationSkript via reticulate
try(source_python("skript_wav_png_transform.py"))
reticulate::repl_python()
import librosa
import librosa.display
from pathlib import Path
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import os
from PyQt5 import QtCore, QtGui, QtWidgets
quit
#Laden von TransformationSkript via reticulate
try(source_python("skript_wav_png_transform.py"))
#transformationsfunktion ausfuehren
precompute_spectrograms()
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
#Bildinhalte klassifizieren und Ergebnisse fuer Visualisierung verarbeiten
try(df_audio <- check_wav(audio_path))
try(df_video <- check_img(video_path))
runApp()
c
runApp()
runApp()
df_audio
df_audio$number <- seq(1,nrow(df_audio))
fig <- plot_ly(df_audio, x=~number, y = ~neutral  , name = 'neutral', type = 'scatter', mode = 'markers')
fig <- fig %>% add_trace(y = ~happy    , name = 'happy', mode = 'markers')
fig <- fig %>% add_trace(y = ~sad  , name = 'sad', mode = 'markers')
fig <- fig %>% add_trace(y = ~angry   , name = 'angry', mode = 'markers')
fig <- fig %>% add_trace(y = ~fear , name = 'fear', mode = 'markers')
fig <- fig %>% add_trace(y = ~disgust , name = 'disgust', mode = 'markers')
fig <- fig %>% add_trace(y = ~surprise   , name = 'surprise', mode = 'markers')
fig <- fig %>% layout(hovermode = "x", xaxis = list(title="Objekt"), yaxis=list(title="Ergebnis"))
fig
fig
df_video$number <- seq(1,nrow(df_video))
fig <- plot_ly(df_video, x=~number, y = ~neutral  , name = 'neutral', type = 'scatter', mode = 'markers')
fig <- fig %>% add_trace(y = ~happy    , name = 'happy', mode = 'markers')
fig <- fig %>% add_trace(y = ~sad  , name = 'sad', mode = 'markers')
fig <- fig %>% add_trace(y = ~angry   , name = 'angry', mode = 'markers')
fig <- fig %>% add_trace(y = ~fear , name = 'fear', mode = 'markers')
fig <- fig %>% add_trace(y = ~disgust , name = 'disgust', mode = 'markers')
fig <- fig %>% add_trace(y = ~surprise   , name = 'surprise', mode = 'markers')
fig <- fig %>% layout(hovermode = "x", xaxis = list(title="Objekt"), yaxis=list(title="Ergebnis"))
fig
fig
runApp()
runApp()
runApp()
runApp()
df_audio
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp('test_video')
runApp('test_video')
runApp('test_video')
getwd()
runApp('video.R')
runApp()
runApp()
wav_name
runApp('video.R')
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
runApp()
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
runApp()
runApp()
runApp()
runApp()
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
runApp()
runApp()
runApp()
try(source_python("skript_video_class.py"))
try(source_python("skript_audio_class.py"))
